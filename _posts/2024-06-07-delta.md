---
layout: post
title: "Native Delta Lake support in DuckDB"
author: "Sam Ansmink"
thumb: "/images/blog/thumbs/240606.svg"
excerpt: "DuckDB now has native support for Delta Lake, an open-source lake house framework. In this blogpost we'll give you a short overview of Delta Lake, Delta Kernel and of course the new DuckDB delta extension."
---

Over the past months, DuckDB Labs has teamed up with Databricks to add native support for Delta Lake to DuckDB using
the new [`delta-kernel-rs`](https://github.com/delta-incubator/delta-kernel-rs) project. Note that if you're already dearly
familiar with Delta Lake and Delta Kernel, or you are just here to know how to boogie, feel free to [skip to the juicy bits](#how-to-delta-in-duckdb).

<!--more-->

## Intro
Delta Lake is an open-source storage framework that enables building a Lakehouse architecture. So to understand Delta Lake,
we need to understand what the Lakehouse architecture is. In simple terms, these a Lakehouse architectures are mostly a
"big pile of data in mixed formats" but with some additional metadata layers on top. These metadata layers generally
aim to provide functionality such as ACID transactions, time travel, partition- and schema evolution, statistics, and much more.
To sum it up in a sentence: Lakehouse architecture allows running various types of data intensive applications
such as data analytics and machine learning applications to run directly on a vast collections of structured, semi-structured and
unstructured data, without the need for an intermediate data warehousing step. If you're ready for the deep dive, we recommend reading the
CIDR 2021 paper ["Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics"](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) by Michael Armbrust et al.
However, if you're (understandably) hesitant to dive into dense scientific literature, this image sums it up pretty well:

<div align="center">
<img src="/images/blog/delta/lakehouse_arch.png"
alt="Image describing Lakehouse architecture"
width="300px"
/></div>
<div align="center">Lakehouse architecture (image source: <a href="https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf#page=2">Armburst et al., CIDR 2021</a>)</div>

## Delta Lake
Now lets zoom in a little on our star of the show for tonight, Delta Lake. Delta Lake is currently one of the leading open-source
Lakehouse formats, along with Apache Iceberg and Apache HUDI.  The easiest way to get a feeling for what Delta Lake is, is to think of
Delta Lake as a "collection of parquet files with some metadata". Then armed with that oversimplification, we will simply create a Delta Table and
examine the files that are created to deepen our understanding a little. To do this, we'll set up Python with the packages: `duckdb`, `pandas` and [`deltalake`](https://github.com/delta-io/delta-rs)
use DuckDB to create some dataframes with test data, then write that to a delta table using the `deltalake` package:

```python
import duckdb
from deltalake import DeltaTable, write_deltalake
con = duckdb.connect();
df1 = con.query("SELECT i as id, i%2 as part, 'value-'||i as value from range(0,5) tbl(i)").df()
df2 = con.query("SELECT i as id, i%2 as part, 'value-'||i as value from range(5,10) tbl(i)").df()
write_deltalake(f"./my_delta_table", df1,  partition_by=["part"])
write_deltalake(f"./my_delta_table", df2,  partition_by=["part"], mode='append')
```

Now that has run, we should have created a basic delta table containing 10 rows, split across 2 partitions that we added in
two separate steps. To double check everything is going to plan lets use DuckDB to query the table:
```shell
SELECT * FROM delta_scan('./my_delta_table') ORDER_BY id";
```

| id   | part  | value    |
|------|-------|----------|
| 0    | 0     | value-0  |
| 1    | 1     | value-1  |
| 2    | 0     | value-2  |
| 3    | 1     | value-3  |
| 4    | 0     | value-4  |
| 5    | 1     | value-5  |
| 6    | 0     | value-6  |
| 7    | 1     | value-7  |
| 8    | 0     | value-8  |
| 9    | 1     | value-9  |

Okay that looks great! All our expected data is there. Now as  
Now lets take a look at what files have actually been created in the
`./my_delta_table` path using `tree ./my_delta_table`:

```text
my_delta_table
├── _delta_log
│   ├── 00000000000000000000.json
│   └── 00000000000000000001.json
├── part=0
│   ├── 0-f45132f6-2231-4dbd-aabb-1af29bf8724a-0.parquet
│   └── 1-76c82535-d1e7-4c2f-b700-669019d94a0a-0.parquet
└── part=1
    ├── 0-f45132f6-2231-4dbd-aabb-1af29bf8724a-0.parquet
    └── 1-76c82535-d1e7-4c2f-b700-669019d94a0a-0.parquet
```

The `tree` output shows 2 different types of files: **data files** in parquet format, and **delta files** in json format. These two files
are the basic structure of a delta table: The data files contain all the data that is stored in the table, and the delta files contain a
log of the changes that have been made to the table. By replaying this log, a reader can construct a valid view of the table. To illustrate this, lets
take a small peek in one of the first delta log file, `my_delta_table/_delta_log/00000000000000000000.json`:
```json
...
{ "add": {
    "path": "part=1/0-f45132f6-2231-4dbd-aabb-1af29bf8724a-0.parquet",
    "partitionValues": { "part": "1" }
  },
  ...
}
{ "add": {
    "path": "part=0/0-f45132f6-2231-4dbd-aabb-1af29bf8724a-0.parquet",
    "partitionValues": { "part": "0" },
  },
  ...
}
...
```
As we can see, this log file contains 2 `add` objects that describe some data being added to respectively the `1` and `0` partitions. Note also
that the partition values themselves are stored in these delta files explicitly, so even though the file structure looks very similar
to a hive-style partitioning scheme, the folder names are not actually used by delta internally, instead the partition values are read from the metadata.

Now with this simple example, we've touched on the basics of how the Delta Lake works. For a more thorough understanding of the internals,
we refer to the [official delta specification](https://github.com/delta-io/delta/blob/master/PROTOCOL.md) which is, by protocol specification standards,
quite easy to read. The official spec describes in detail how Delta handles every detail, from the basics described here to more complex things like checkpointing, deletion vectors, schema evolution,
and much more.

## Implementation
### The Delta Kernel
Implementing a relatively complex protocol such as Delta Lake, requires significant development and maintenance effort. For this reason, when looking to
add support for such a protocol to an engine, the logical choice would be to look for a complete library to take care of this. In the case of Delta, we could
opt for the [`delta-rs` library](https://github.com/delta-io/delta-rs). However, when it comes to implementing a native DuckDB Delta extension, this is problematic:
If we were to use the `delta-rs` library for implementing the DuckDB extension, all interaction with the Delta tables would go through the `delta-rs` library. But remember,
a Delta table is effectively "just a bunch of parquet files with some metadata". This would mean that when DuckDB wants to read a Delta table,
the data files will be read by the `delta-rs` parquet reader, using the`delta-rs` filesystems. But that's annoying: DuckDB already comes [shipped]() with
an excellent parquet reader. Also, DuckDB already has support for a [variety of filesystems]() with its own [credential management system]().
What would be ideal, is if there were some library that implements **only the Delta protocol** while letting DuckDB handle all the things it already knows how to handle!

Well this is exactly what the [delta-kernel-rs library](https://github.com/delta-incubator/delta-kernel-rs) offers. Originating in [Java land](https://github.com/delta-io/delta/tree/master/kernel),
the Delta Kernel is a "set of libraries for building Delta connectors that can read from and write into Delta tables without the need to understand the Delta protocol details".
The Rust-based delta-kernel-rs library that DuckDB's delta extension relies on, has recently launched its v0.1.0 version and while still experimental, already offers a lot of functionality. Because delta-kernel-rs
exposes a C/C++ ffi, integrating it into a DuckDB extension has been very straightforward.

### DuckDB Delta extension
Now we'll take a look into how the `delta_scan` function of the Delta Extension has been implemented. To understand this, we need to establish the 4 main components involved:

| component         | description                         |
|-------------------|-------------------------------------|
| Delta kernel      | The delta-kernel-rs library         |
| Delta extension   | DuckDB's loadable delta extension   |
| Parquet extension | DuckDB's loadable parquet extension |
| DuckDB            | Super cool analytical database      |

Additionally, we need to understand that there are 4 main APIs involved:

| API                    | description                                     |
|------------------------|-------------------------------------------------|
| FileSystem API         | DuckDB's API for I/O                            |
| TableFunction API      | DuckDB's API for table producing functions      |
| MultiFileReader API    | DuckDB's API for handling multi-file scans      |
| Delta Kernel C/C++ FFI | Delta Kernel FFI into the Rust libary for Delta |

Now when a `delta_scan` table function is executed by DuckDB. DuckDB will call into the Delta extension over the TableFunction API. Next, the Delta extension will then call into the parquet extension
also using the TableFunction API, to inject a custom MultiFileReader. This custom `DeltaMultiFileReader` will then drive the parquet scan by providing with a list of files over the MultiFileReader API.
Finally, whenever the parquet extension requires any IO, it will call into DuckDB to handle the I/O. This interaction between the different components is shown in the figure below.

<!-- TODO add a block diagram demonstrating the different components involved in a deltascan. () -->

<img src="/images/blog/delta/delta_ext_overview.svg"
alt="Diagram showing operation of delta_scan"
width="300px"
/>

While there are obviously some important details missing here, such as handling deletion vectors and column mappings, this simplified view gives a good idea of the inner workings.
More importantly, it demonstrates how the current implementation achieves good logical separation, with the 4 components working together across 4 clearly defined APIs.

| component         | task                                                        |
|-------------------|-------------------------------------------------------------|
| Delta kernel      | Processing of Delta Metadata                                |
| Delta extension   | Mapping of the Delta Kernel FFI to the MultiFileReader API  |
| Parquet extension | Scanning the parquet files                                  |
| DuckDB            | Everything else                                             |

To conclude, by implementing delta in DuckDB through a separate, delta kernel-based extension, we achieve the following desirable properties:

**The details of the Delta Lake protocol remain largely opaque to any DuckDB component.** The only point of contact is the narrow
FFI exposed by the Delta kernel. This keeps the complexity in the delta extension to an absolute minimum.

**Full reuse of exsisting Parquet scanning logic** of DuckDB, without any code reuse or compile time dependency. The interaction between the Delta and Parquet extension go through
the TableFunction API.

**All I/O will go through DuckDBs FileSystems.** This means that all [FileSystems]() that are available to DuckDB, are available to scan with delta. This means that no special handling is required for
Delta authentication.
**TODO**: this is not true YET, we still need to implement this actually.


- DuckDB can completely reuse all existing parquet scanning code without compile time dependencies
- The complexity of scanning the parquet files is handled fully by the Parquet extension

The kernel is responsible for handling the delta kernel protocol, whilst

## How to Delta in DuckDB

Using the delta extension in DuckDB is very simple, as it is distributed as one of the core DuckDB extensions, and available for [autoloading](/docs/extensions/overview#autoloading-extensions).
What this means is that you can simply start DuckDB (using v0.10.3 or higher) and run:

```sql
SELECT * FROM delta_scan('./my_delta_table');
```

DuckDB will automatically install the correct version of the delta extension and load it into DuckDB, then it will query the local delta table `./my_delta_table`.

In the (quite probable) case that your delta table lives on S3, there are likely some S3 credentials that you need to set. Are your credentials already in
one of the [default places](https://github.com/aws/aws-sdk-cpp/blob/main/docs/Credentials_Providers.md) AWS tooling will check for credentials? For exampe in an environment variable, or in the `~/.aws/credentials` file? Simply run:

```sql
CREATE SECRET delta_s1 (
    TYPE S3,
    PROVIDER CREDENTIAL_CHAIN
)
SELECT * FROM delta_scan('s3://some-bucket/path/to/a/delta/table');
```

Do you prefer remembering your AWS tokens by heart, and would like to type them out? Go with:

```sql
CREATE SECRET delta_s2 (
    TYPE S3,
    KEY_ID, 'AKIAIOSFODNN7EXAMPLE',
    SECRET, 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
    REGION 'eu-west-1'
)
SELECT * FROM delta_scan('s3://some-bucket/path/to/a/delta/table');
```

Do you have multiple delta tables, with different credentials? No problem, you can use scoped secrets:
```sql
CREATE SECRET delta_s3 (
    TYPE S3,
    KEY_ID, 'AKIAIOSFODNN7EXAMPLE1',
    SECRET, 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY1',
    REGION 'eu-west-1',
    SCOPE 's3://some-bucket1'
)
CREATE SECRET delta_s4 (
    TYPE S3,
    KEY_ID, 'AKIAIOSFODNN7EXAMPLE2',
    SECRET, 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY2',
    REGION 'us-west-1',
    SCOPE 's3://some-bucket2'
)
SELECT * FROM delta_scan('s3://some-bucket1/table1');
SELECT * FROM delta_scan('s3://some-bucket2/table2');
```

Finally, is your table public, but outside the default AWS region? Make sure you set the region using an empty S3 Secret:

```sql
CREATE SECRET delta_s5 (
    TYPE S3,
    REGION 'eu-west-2',
)
SELECT * FROM delta_scan('s3://some-public-bucket/table1');
```

## Current state of the Delta Extension
Currently, the delta Extension is still considered experimental. This is partly because the delta extension itself is still very new,
but also because the delta-kernel-rs project it relies on is still experimental. Nevertheless, many of the core features
involved in ready Delta tables are already supported by the current latest version of the delta extension, such as:
- All data types
- Filter and projection pushdown
- Deletion vectors
- File skipping based on filter pushdown
- Partitioned tables

Architecture wise, the delta extension is supported on Linux (x64) and MacOS (x64 and arm64). Support for the remaining (linux arm, windows, windows R) DuckDB core platforms is coming
soon. Additionally, we will continue to work together with Databricks on further improving the Delta Extension to add more features like
- Write support
- Column mapping
- Time travel
- Variant, RowIds

## Conclusion
**TODO**: wrap up into a story
- Presented a new DuckDB extension
- Given a crash course in Delta Lake
- Emphasize how effective building on the delta kernel has been so far
- announce (again) Hannes' talk on DAIS
- announce (again) Nicks talk on DAIS
