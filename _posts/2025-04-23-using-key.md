---
layout: post
title: "USING KEY in Recursive CTEs"
author: Björn Bamberg
thumb: "/images/blog/thumbs/duckdb-dbt.svg"
image: "/images/blog/thumbs/duckdb-dbt.png"
excerpt: "Recursive CTEs in SQL allow for powerful iterative queries like graph traversals but can be memory-intensive and slow due to repeated row accumulation. DuckDB’s new `USING KEY` feature solves this by treating the recursion result as a keyed dictionary instead of a growing list—updating existing entries instead of appending. This leads to dramatically better performance and memory efficiency, especially in graph algorithms like shortest path and distance-vector routing. It also simplifies logic by giving direct access to the latest state via the recurring table."
tags: ["using DuckDB"]
---


# The Beauty of a CTE

As SQL queries become more complex, managing their readability, modularity and reusability becomes increasingly difficult. Common Table Expressions (CTEs) were introduced to address these issues by allowing developers to define temporary, named result sets within a query. Similar to functions in programming, CTEs allow a large query to be broken down into logical building blocks, making it easier to understand, maintain and debug.

CTEs are particularly useful for structuring multi-step transformations that might otherwise require deeply nested subqueries or complex joins. By improving both the clarity and structure of SQL code, CTEs have become an essential tool in modern query writing - enabling the clear, declarative expression of even the most sophisticated logic.

# Improving CTEs Through Recursion

To enhance the expressive power of SQL, recursive CTEs were introduced in the SQL:1999 standard. These allow a query to reference the results from previous iterations within the same expression, enabling SQL to solve more complex problems such as graph traversal and other iterative computations.

This capability pushes SQL beyond basic data retrieval, allowing for the formulation of complex, iterative logic directly in SQL. In fact, recursive CTEs make SQL Turing-complete, meaning it can theoretically express any computation (given sufficient time and memory).

*_But how do recursive CTEs work in DuckDB?_*

Let’s look at a simple example to break down the mechanism. Suppose we want to calculate the largest power of 2 that is smaller than 100. We can use a recursive CTE to generate powers of 2 iteratively until we reach that limit:

```sql
WITH RECURSIVE power(a, b) AS (
	SELECT 2, 1
		UNION
	SELECT a, a * b
	FROM power
	WHERE a * b < 100
)
SELECT *
FROM power;
```

This query computes every power of 2 (in the form `a = a * b`). We can divide a recursive CTE into two parts, separated by the `UNION` keyword. The part above the `UNION` is the **non-recursive part** (`SELECT 2, 1` in our example), and the part below is the **recursive part**.

In the recursive part, the CTE references itself. This self-reference points to what we call the **_working table_**. The working table always holds the rows produced in the **previous iteration**.

Here's how it works step by step:

- First, the **non-recursive part** is executed, producing the initial rows. These are stored in the working table—for example, the row with value `2, 1`.
- Then, the **recursive part** is executed using the rows from the working table. Every new row the recursive part produces is added to the **_intermediate table_**, which holds results from the current iteration.
- If the intermediate table is **empty**, recursion ends.
- Otherwise, we clear the working table and replace it with the contents of the intermediate table—preparing for the next iteration.
- Each time, we also add the contents of the intermediate table to the **_union table_**, which accumulates all results across iterations.

Here you can see the enteries of the tables in each iteration:

| **Iteration** | **Output of Recursive Step** |  **Working Table**   | **Intermediate Table** | **Union Table**      |
|---------------|------------------------------|----------------------|------------------------|----------------------|
| 0             | SELECT 2, 1                  | ∅                    | (2, 1)                 | (2, 1)               |
| 1             | 2 * 1 = 2                    | (2, 1)               | (2, 2)                 | (2, 1) <br>(2, 2)               |
| 2             | 2 * 2 = 4                    | (2, 2)               | (2, 4)                 | (2, 1) <br>(2, 2) <br> (2, 4)               |
| 3             | 2 * 4 = 8                    | (2, 4)               | (2, 8)                 | (2, 1) <br>...<br> (2, 8)               |
| 4             | 2 * 8 = 16                   | (2, 8)               | (2, 16)                | (2, 1) <br>...<br> (2, 16)              |
| 5             | 2 * 16 = 32                  | (2, 16)              | (2, 32)                | (2, 1) <br>...<br> (2, 32)              |
| 6             | 2 * 32 = 64                  | (2, 32)              | (2, 64)                | (2, 1) <br>...<br> (2, 64)              |
| 7             | 2 * 64 = 128                 | (2, 64)              | ∅ *stop*               | (2, 1) <br>...<br> (2, 64)              |

When the recursive CTE completes, the union table holds the entire result set, including all intermediate rows from every iteration. This can lead to unnecessary overhead, especially if all we need is the last row — the final result of the recursion. Storing every intermediate value can be inefficient, particularly when working with large datasets or when the intermediate results aren't required.

```
┌───────┬───────┐
│   a   │   b   │
│ int32 │ int32 │
├───────┼───────┤
│     2 │     1 │
│     2 │     2 │
│     2 │     4 │
│     2 │     8 │
│     2 │    16 │
│     2 │    32 │
│     2 │    64 │
└───────┴───────┘
```

# USING (a) KEY To Improve Recursive CTEs

The growth of the union table is a key characteristic of recursive CTEs. In each iteration, newly produced rows are appended to the union table. While this is fundamental to how recursion accumulates results, it can lead to significant **memory usage** and **performance overhead**, particularly with large datasets or complex queries.

This becomes problematic when we are only interested in keeping the **latest** version of each row, rather than all historical versions. To address this, we are introducing a new feature in DuckDB that enhances the **performance** and **expressiveness** of recursive CTEs by changing how intermediate results are managed.

It introduces two major differences compared to traditional recursive CTEs:

1. **Direct access to the union table** — now called the **_recurring table_** in this context.
2. Instead of simply appending new rows to the recurring table, it now functions more like a **dictionary** (similar to a Python `dict`), allowing key-based updates.

To use this new feature, add the `USING KEY` clause to your recursive CTE.

```sql
WITH RECURSIVE tbl(a,b,c) USING KEY (a) AS (
    ...
)
```

For `USING KEY`, the schema of the recursive CTE is divided into **key columns** and **payload columns**. The key columns are specified using the `USING KEY (<column_names>)` clause, while the remaining columns are treated as payload.

This distinction changes how the recurring table behaves. Instead of blindly appending new rows on each iteration, it acts more like a dictionary. If a new row has a key combination that hasn't been seen before, it's added to the recurring table as usual. But if a row shares a key with an existing entry, the payload is updated instead of added—replacing the previous values for that key.

This approach allows recursive queries to maintain and update state more efficiently, especially in algorithms where keeping the latest or best value for a given key is important.

Let's consider a similar example.

```sql
WITH RECURSIVE power(a,b) USING KEY (a) AS (
    SELECT *
    FROM (VALUES (2, 1), (3, 1))
        UNION
    SELECT a, a * b
    FROM power
    WHERE a * b < 100
)
SELECT *
FROM power;
```
We start with two rows in the **non-recursive part**: one with a key of `2` and another with a key of `3`. In the **recursive part**, we multiply the key by the payload column. This computation again produces two rows with the same keys, `2` and `3`.

However, unlike traditional recursive CTEs, we **do not append** these new rows to the union table. Instead, we **update the existing rows** with key 2 and 3 in the **recurring table**, modifying only the payload values. As a result, we still have just two rows—one for each unique key.

| **Iteration** | **Output of Recursive Step**  |  **Working Table**   | **Intermediate Table** | **Union Table**      |
|---------------|-------------------------------|----------------------|------------------------|----------------------|
| 0             | SELECT 2, 1 <br> SELECT 3, 1  | -                    | (2, 1) <br> (3, 1)     | (2, 1) <br> (3, 1)   |
| 1             | 2 * 1 = 2 <br> 3 * 1 = 3      | (2, 1) <br> (3, 1)   | (2, 2) <br> (3, 3)     | (2, 2) <br> (3, 3)   |
| 2             | 2 * 2 = 4 <br> 3 * 3 = 9      | (2, 2) <br> (3, 3)   | (2, 4) <br> (3, 9)     | (2, 4) <br> (3, 9)   |
| 3             | 2 * 4 = 8 <br> 3 * 9 = 27     | (2, 4) <br> (3, 9)   | (2, 8) <br> (3, 27)    | (2, 8) <br> (3, 27)  |
| 4             | 2 * 8 = 16 <br> 3 * 27 = 81   | (2, 8) <br> (3, 27)  | (2, 16) <br> (3, 81)   | (2, 16) <br> (3, 81) |
| 5             | 2 * 16 = 32 <br> 3 * 81 = 243 | (2, 16) <br> (3, 81) | (2, 32)                | (2, 32) <br> (3, 81) |
| 6             | 2 * 32 = 64                   | (2, 32)              | (2, 64)                | (2, 64) <br> (3, 81) |
| 7             | 2 * 64 = 128                  | (2, 64)              | - (stop)               | (2, 64) <br> (3, 81) |

As we can see, the growth of the union table is **significantly slower** than before, which leads to **reduced memory usage**.
```
┌───────┬───────┐
│   a   │   b   │
│ int32 │ int32 │
├───────┼───────┤
│     2 │    64 │
│     3 │    81 │
└───────┴───────┘
```
This behavior is especially useful with algorithms in which we are interested in the **latest**, **best**, or **smallest** value for a given key. The maximum number of rows in the union table is now **bounded by the number of unique keys**—a powerful advantage when working with large datasets.

The second major difference is that we can now reference the **recurring table** directly in the recursive part of the CTE. This allows us to access the most recent values without keeping the needed rows in an extra column.

```sql
WITH RECURSIVE cte(a,b) USING KEY (a) AS (
  ...
  FROM recurring.cte
)
```

# Where It Makes a Difference

To highlight the difference between the two approaches, let's consider a more complex example using a graph dataset—a social network graph. In this dataset, nodes represent people, and edges correspond to relationships between them. The table we’re working with is called `knows`, where each row consists of two IDs representing two people who know each other.

Our goal is to find the shortest path between two people within this social network, which requires us to traverse the graph. We begin by starting at a given person and recursively explore all the individuals they know, continuing until we reach the target person.

To optimize the search and avoid unnecessary traversals, we can stop further searches once the target person is found. This is achieved by adding an endReached column to the recursive CTE. This column is set to `TRUE` when the target person is encountered and `FALSE` otherwise. We can then use the `bool_or` function to check if any of the recursive paths successfully reach the target person, enabling us to stop additional traversal once the goal is met.

```sql
WITH RECURSIVE paths(startNode, currNode, via, path, toReach, endReached, found) AS (
    SELECT
         n1.id AS startNode, n1.id AS currNode, NULL :: HUGEINT AS via, 0 AS path, n2.id AS toReach, FALSE  AS endReached, FALSE
      FROM Person AS n1, Person AS n2
      WHERE n2.id <> n1.id
    UNION ALL
         paths.startNode AS startNode,
         person2id AS currNode,
         COALESCE(via, person2id) AS via, path+1,
         toReach,
         bool_or(person2id = toReach)
             OVER (PARTITION BY toReach, startNode ROWS BETWEEN UNBOUNDED PRECEDING
                            AND UNBOUNDED FOLLOWING) AS endReached,
         (person2id = toReach)
      FROM paths
      JOIN knows ON paths.currNode = person1id
       AND NOT paths.endReached
 )
 SELECT startNode, toReach, via, path
 FROM paths WHERE found ORDER BY path, startNode, toReach;
```

If we have a large graph with many edges, the **union table** in a recursive CTE can grow very large, potentially exceeding memory limits. This not only causes significant **performance issues** but can even lead to **query crashes** in extreme cases.

With the new `USING KEY` feature, we can avoid this problem by changing how updates are handled. This enables a range of new algorithms to be expressed efficiently in SQL, including algorithms for finding the shortest paths in large graphs.

One example is the **Distance Vector Routing (DVR)** algorithm, a distributed method for computing the shortest paths in a network. In this setup:

- Each node maintains a **routing table** that records the cost to reach other nodes.
- The recurring table stores the current best-known paths for each node.
- When a shorter path to a node is found, the corresponding routing table entry is updated.
- These updates are distributed to neighboring nodes via the **working table**.

To check if a newly incoming update improves the current path, we perform a **lookup** in the recurring table. If the new cost is smaller than the existing one, we update the entry and propagate the update. This mechanism allows efficient, memory-safe pathfinding even in very large graphs—something that would be impractical with traditional recursive CTEs.

```sql
WITH RECURSIVE
    dvr(here, there, via, cost) USING KEY (here, there) AS (
      -- Start with DVR tables for all nodes, a table is identified by the here column.
      SELECT n.person1id, n.person2id, n.person2id :: string, 1:: double precision
      FROM   knows AS n
        UNION
      (SELECT n.person1id, dvr.there, dvr.here, (1 + dvr.cost) :: double precision AS cost
      FROM dvr -- Working table - Changes shared by neighbors.
      JOIN knows AS n -- Edges between the routes - needed to know how much weight the edge has.
          ON  n.person2id = dvr.here  -- SEND the update only to those tables that have an edge to sender.
          AND n.person1id != dvr.there -- Does not need an entry for itself.
      LEFT JOIN recurring_dvr AS rec -- Recurring table - The current routing tables.
          ON rec.here = n.person1id AND rec.there = dvr.there -- Update only routing table entries where the edge from the receiver starts.
      WHERE (1 + dvr.cost) :: double precision < COALESCE(rec.cost, 'Infinity' :: double precision) --  The updated cost + cost of edge must be less than the current entry in the routing table.
      ORDER BY 1 + dvr.cost ASC)
    )
  SELECT *
  FROM dvr ORDER BY cost, here, there ASC;
```

To highlight the performance difference between the two approaches, we created five graphs of increasing size and measured the number of rows processed during recursion. We compared the standard recursive CTE approach (REC) with the new USING KEY approach (UK).

<div align="center" style="margin:10px">
    <a href="/images/blog/using-key/table.svg">
        <img
          src="/images/blog/using-key/table.svg"
          alt="Transformation Layer Data Model"
          width=700
        />
    </a>
</div>

Even for the smallest graph, Graph A, the difference is significant: the REC method produces around 350,000 rows, while the UK method generates only 700 rows. As the graph size increases, the gap becomes even more striking. In Graph C, with 484 nodes and 1,446 edges, the REC approach processes nearly 1 billion rows, while the UK method handles just 20,000 rows.

Although this isn't the largest graph in our benchmark, the REC approach is already approaching out-of-memory (OOM) conditions

This substantial difference in memory usage is only part of the story. The performance of the REC approach also degrades quickly. While both methods perform similarly on small graphs, REC becomes significantly slower as the graph grows—eventually crashing—whereas UK continues to scale smoothly.

<div align="center" style="margin:10px">
    <a href="/images/blog/using-key/plot.svg">
        <img
          src="/images/blog/using-key/plot.svg"
          alt="Transformation Layer Data Model"
          width=700
        />
    </a>
</div>

And that’s the beauty of the new USING KEY feature. It enables more efficient expression of complex recursive algorithms by minimizing memory usage and improving runtime performance. If you're working with recursive CTEs in DuckDB, be sure to take advantage of this powerful addition—it can make a significant difference in your queries.