---
layout: post
title: "Vector Similarity Search in DuckDB – Part 2"
author: "Max Gabrielsson"
thumb: "/images/blog/thumbs/vss-2.svg"
image: "/images/blog/thumbs/vss-2.png"
excerpt: ""
---

What's new in DuckDB-VSS?

In the previous blog post, we introduced the DuckDB Vector Similarity Search (VSS) extension. While the extension is still quite experimental, we figured it would be nice to share some of the new features and improvements that have been added since the initial release.

### Indexing Speed Improvements

We've previously documented that creating a HNSW index over a already populated table is much more efficient than first creating the index and then inserting into the table. This is because it is much easier to divide the work into chunks large enough to distribute over multiple threads. However, in the initial release this work distribution was a bit too coarse-grained and required the table to contain a ton of data for additional worker threads to be scheduled. We've now introduced an extra buffer step in the index creation pipeline which enables more fine-grained work distribution, smarter memory allocation and less contention between worker threads. This results in much higher CPU saturation and a significant speedup when building HNSW indexes in environments with many threads available.

Another bonus of this change is that we can now emit a progress bar when building the index, which is a nice touch when waiting for index creation to finish despite the better use of system resources.

### New distance functions

In the initial release of VSS we supported three different distance functions: `array_distance`, `array_cosine_similariy` and `array_inner_product` when re-writing queries to use the `HNSW` index. However, only the `array_distance` function is actually a _distance_ function in that it returns results closer to 0 when the vectors are similar, and close to 1 when they are dissimilar, in contrast to e.g. `array_cosine_similarity` that returns 1 when the vectors are identical. Oops! 

To remedy this we've introduced the `array_cosine_distance` (equivalent to `1 - array_cosine_simililarity`) and `array_negative_inner_product` (equivalent to `-array_inner_product`). These will now be accelerated with the use of the `HNSW` index instead, making the query patterns and ordering consistent for all supported metrics regardless if you make use of the `HNSW` index or not. 

Additionally if you have a `HNSW` using the e.g. the `cosine` metric and write a top-k style query using `1 - array_cosine_similarity` as the ranking criteria, the optimizer should be able to normalize the expression to `array_cosine_distance` and use the index for this function as well.

For completeness we've also added the equivalent distance functions for the dynamically sized `LIST` datatype (prefixed with `list_` instead of `array_`) and changed the `<=>` binary operator to now be an alias of `array_cosine_distance`, matching the semantics of the `pgvector` extension for PostgreSQL

### Index accelerated "Top-K" aggregates

Another cool thing thats happened in DuckDB core since last time is that DuckDB now has extra overloads for the `min_by`/`max_by` (and their aliases `arg_min`/`arg_max`) aggregate function that take an optional third `n` argument, which specifies the number of top-k (or top `n`) elements to keep and outputs them into a sorted `LIST` value. 

The VSS extension now includes optimizer rules to use to the `HNSW` index to accelerate these aggregates when the ordering input is a distance function that references a indexed vector column, similarly to the `SELECT a FROM b ORDER BY array_distance(a.vec, query_vec) LIMIT k` query pattern that we discussed in the previous blog post. These new overloads allow you to express the same query in a more concise and readable way, while still avoiding the need for a full scan and sort of the underlying table. (That is, as long as the table has a matching `HNSW` index).

### Index accelerated LATERAL JOIN's

After running some benchmarking on the initial version of VSS we realized that even though index-lookups on our `HNSW` index is really fast (thanks to the USearch library that it is based on!), using DuckDB to search for individual vectors at a time has a lot of latency compared to other solutions. The reasons for this are many and nuanced, but we want to be clear that our choice of HNSW implementation, USearch, is not the bottleneck here as profiling revelead only about 2% of the runtime is actually spent inside of usearch. 

Rather, most of the per-query overhead comes from the fact that DuckDB is just not optimized for "point" queries, i.e. queries that only really fetch and process a single row. Because DuckDB is based on a "vectorized" execution an engine, the smallest "unit of work" is not 1 row, but 2048, and because we expect to crunch through a ton of data we generally favor spending a lot of time up front to optimize the query plan and pre-allocate large buffers and caches so that everything is as efficient as possible once we start executing. But a lot of this work becomes unneccessary when the actual working set is so small. For example, is it really worthwile to inspect and hash every single element of a constant 768-long query vector to attempt to look for common subexpressions if you know there is only going to be a handful of rows in the result?

While we have some ideas on how to improve this scenario in the future, we decided to take another approach for now and instead try focus not on our weaknesses, but on our strengths. That is, crunching through a ton of data! Instead of trying to optimize the "1:N", "given this one embedding, give me the closes N embeddings" query, what if we instead focused on the "N:M", "given all these N embeddings, pair them up with the closest M embeddings each". What would that look like? Well, that would be a `LATERAL` join of course!

Basically, we are now able to make use of `HNSW` indexes to accelerate LATERAL joins where the "inner" query looks just like the "top-k" style queries we normally accelerate e.g `SELECT a FROM b ORDER BY array_distance(a.vec, query_vec) LIMIT k`. but where the `query_vec` is a reference to the "outer" join table. The only requirement is of course that the inner table has a `HNSW` index on the vector column matching the distance function. Here's an example:

```sql
-- Set the random seed for reproducibility
SELECT setseed(0.42);

-- Create some example tables
CREATE TABLE queries AS SELECT i as id, [random(), random(), random()]::FLOAT[3] as embedding 
FROM generate_series(1, 10000) r(i);

CREATE TABLE items AS SELECT i as id, [random(), random(), random()]::FLOAT[3] as embedding
FROM generate_series(1, 10000) r(i);

-- Collect the 5 closest items to each query embedding
SELECT queries.id as id, list(inner_id) as matches 
    FROM queries, LATERAL (
        SELECT items.id as inner_id, array_distance(queries.embedding, items.embedding) as dist
        FROM items 
        ORDER BY dist 
        LIMIT 5
    )
GROUP BY queries.id
```

Executing this on my Apple M3 Pro Macbook with 36gb ram takes about 10s.

If we `EXPLAIN` this query plan, we'll see a lot of advanced operators, but the most worrysome among these is the CROSS_PRODUCT which blows up the expected cardinality and is a sign that we are doing a lot of work that we probably don't want to do. 

```sql
pragma explain_output='optimized_only';

┌─────────────────────────────┐
│┌───────────────────────────┐│
││  Optimized Logical Plan   ││
│└───────────────────────────┘│
└─────────────────────────────┘
┌───────────────────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│             id            │
│          matches          │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│__internal_decompress_integ│
│     ral_bigint(#0, 1)     │
│             #1            │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         AGGREGATE         │
│    ────────────────────   │
│         Groups: id        │
│                           │
│        Expressions:       │
│       list(inner_id)      │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│             #0            │
│             #1            │
│             #2            │
│             #3            │
│__internal_compress_integra│
│     l_usmallint(#4, 1)    │
│                           │
│       ~1000000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         DELIM_JOIN        │
│    ────────────────────   │
│      Join Type: INNER     │
│                           │
│        Conditions:        ├───────────────────────────────────────────┐
│ (embedding IS NOT DISTINCT│                                           │
│       FROM embedding)     │                                           │
│                           │                                           │
│       ~1000000 Rows       │                                           │
└─────────────┬─────────────┘                                           │
┌─────────────┴─────────────┐                             ┌─────────────┴─────────────┐
│           FILTER          │                             │          SEQ_SCAN         │
│    ────────────────────   │                             │    ────────────────────   │
│        Expressions:       │                             │          queries          │
│    (limit_rownum <= 5)    │                             │                           │
│                           │                             │                           │
│       ~1000000 Rows       │                             │         ~1000 Rows        │
└─────────────┬─────────────┘                             └───────────────────────────┘
┌─────────────┴─────────────┐
│           WINDOW          │
│    ────────────────────   │
│        Expressions:       │
│     ROW_NUMBER() OVER     │
│  (PARTITION BY embedding  │
│   ORDER BY array_distance │
│ (queries.embedding, items │
│ .embedding) ASC NULLS LAST│
│   ROWS BETWEEN UNBOUNDED  │
│ PRECEDING AND CURRENT ROW)│
│                           │
│       ~5000000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│          inner_id         │
│            dist           │
│         embedding         │
│                           │
│       ~5000000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│       CROSS_PRODUCT       │
│    ────────────────────   ├──────────────┐
│       ~5000000 Rows       │              │
└─────────────┬─────────────┘              │
┌─────────────┴─────────────┐┌─────────────┴─────────────┐
│          SEQ_SCAN         ││         DELIM_GET         │
│    ────────────────────   ││    ────────────────────   │
│           items           ││                           │
│                           ││                           │
│        ~10000 Rows        ││         ~500 Rows         │
└───────────────────────────┘└───────────────────────────┘
```

However, if we add a `CREATE INDEX my_hnsw_idx ON items USING HNSW(embedding)` and re-explain, we get this plan instead:

```sql
┌─────────────────────────────┐
│┌───────────────────────────┐│
││  Optimized Logical Plan   ││
│└───────────────────────────┘│
└─────────────────────────────┘
┌───────────────────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│             id            │
│          matches          │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│__internal_decompress_integ│
│     ral_bigint(#0, 1)     │
│             #1            │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         AGGREGATE         │
│    ────────────────────   │
│         Groups: id        │
│                           │
│        Expressions:       │
│       list(inner_id)      │
│                           │
│        ~500000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│             #0            │
│             #1            │
│             #4            │
│             #2            │
│__internal_compress_integra│
│     l_usmallint(#3, 1)    │
│                           │
│       ~1000000 Rows       │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│         PROJECTION        │
│    ────────────────────   │
│        Expressions:       │
│          inner_id         │
│         embedding         │
│             #3            │
│             #4            │
│             #2            │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│      HNSW_INDEX_JOIN      │
│    ────────────────────   │
└─────────────┬─────────────┘
┌─────────────┴─────────────┐
│          SEQ_SCAN         │
│    ────────────────────   │
│          queries          │
│                           │
│         ~1000 Rows        │
└───────────────────────────┘
```

We can see that this plan is drastically simplified, but most importantly, the new `HNSW_INDEX_JOIN` operator replaces the `CROSS_PRODUCT` node that was there before. Executing this query now takes about 0.15s. Thats almost a 66x speedup!

This optimization was just recently added to the VSS extension, so if you've already installed `vss` for DuckDB v1.1.2 you'll need to run `FORCE INSTALL vss` to get the latest version.

### Conclusion

Thats all for this time folks! We hope you've enjoyed this update on the DuckDB Vector Similarity Search extension. While this update has focused on a lot of new features we're still working on improving some of the limitations mentioned in the previous blog post and hope to have more to share soon related to custom indexes and index-based optimizations in general. If you have any questions or feedback, feel free to reach out to us on the [duckdb-vss GitHub repository](https://github.com/duckdb/duckdb_vss) or on the DuckDB discord channel. Hope to see you around!

```sql